{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-9-47a505ca9155>, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-47a505ca9155>\"\u001b[1;36m, line \u001b[1;32m45\u001b[0m\n\u001b[1;33m    inputs.append(x)\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow.contrib.learn as learn\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "def get_model_dir(name,erase):\n",
    "    base_path = os.path.join(\".\",\"dnn\")\n",
    "    model_dir = os.path.join(base_path,name)\n",
    "    os.makedirs(model_dir,exist_ok=True)\n",
    "    return model_dir\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "#save init (?)\n",
    "#sess = tf.Session()\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "#builder = tf.saved_model.builder.SavedModelBuilder(\"./model\")\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(tf.VERSION)\n",
    "\n",
    "df = pd.read_csv('../NNNormalizeData-out.csv')\n",
    "\n",
    "np.random.seed(42) # always shuffle the same way \n",
    "df = df.reindex(np.random.permutation(df.index)) # shuffle examples \n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "inputs = []\n",
    "target = []\n",
    "\n",
    "y=0;    \n",
    "for x in df.columns:\n",
    "    if y != 35 :\n",
    "        inputs.append(x)\n",
    "    else :\n",
    "        target.append(x)\n",
    "    y+=1\n",
    "\n",
    "total_inputs,total_output = df.as_matrix(inputs).astype(np.float32),df.as_matrix([target]).astype(np.int32)\n",
    "\n",
    "train_inputs, test_inputs, train_output, test_output = train_test_split(total_inputs, total_output, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Total number of examples %d\" %(total_inputs.shape[0]+1))\n",
    "print(\"Train number of examples %d\" %(train_inputs.shape[0]+1))\n",
    "print(\"Test number of examples %d\" %(test_inputs.shape[0]+1))\n",
    "\n",
    "\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=train_inputs.shape[1])]\n",
    "#target_column = [tf.contrib.layers.real_valued_column(\"output\", dimension=train_output.shape[1])]\n",
    "model_dir = get_model_dir('ModelSave',True)\n",
    "\n",
    "training_steps = 20000\n",
    "save_checkpoints_steps=400 # used to run the early stopping monitor \n",
    "early_stopping_rounds=2000   # after 50 SPETS if \"loss\" doesn't improve stop the train\n",
    "batch_size=(train_inputs.shape[0]+1)/10 # train number of examples / 10 - 10 batch = 1 epoch\n",
    "\n",
    "# The hyperparameters specified here will be searched.  Every combination.\n",
    "\n",
    "\n",
    "classifier = learn.DNNClassifier(hidden_units=[100, 50, 20], n_classes=5\n",
    "                                 ,model_dir= model_dir\n",
    "                                 #,optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "                                 #     learning_rate=0.01)\n",
    "                                 #     l1_regularization_strength=0.001\n",
    "                                 #     )\n",
    "                                 ,config=tf.contrib.learn.RunConfig(save_checkpoints_steps=save_checkpoints_steps\n",
    "                                                                    ,save_checkpoints_secs=None)\n",
    "                                 ,feature_columns=feature_columns)\n",
    "\n",
    "\n",
    "#early stopping using validation monitor \n",
    "print(\"configuring early stopping\")\n",
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    x=test_inputs,\n",
    "    y=test_output,\n",
    "    #every_n_steps=200,  # when to run the monitor - not working - forcing with save_checkpoints_steps\n",
    "    early_stopping_metric=\"loss\",         #\"accuracy\" of \"loss\"\n",
    "    early_stopping_metric_minimize=True,     #False Maximize accuracy (True is minimize applied to loss)  \n",
    "    early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "print(\"Fit will start...\")\n",
    "# Startup grid search\n",
    "threads = multiprocessing.cpu_count()\n",
    "print(\"Using {} cores.\".format(threads))\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "    ,'batch_size': [64, 128, 256, batch_size]\n",
    "    #,'steps': [training_steps]\n",
    "}\n",
    "\n",
    "grid_classifier = GridSearchCV(classifier,verbose=True, n_jobs=threads,scoring='accuracy', \n",
    "                             param_grid=param_grid ,fit_params={'monitor':validation_monitor})\n",
    "\n",
    "\n",
    "\n",
    "#classifier = learn.SKCompat(classifier) # For Sklearn compatibility\n",
    "grid_classifier.fit(train_inputs, train_output)\n",
    "print(\"Fit is finish...\")\n",
    "\n",
    "\n",
    "#print (classifier.get_variable_names()) \n",
    "\n",
    "#Save Model into saved_model.pbtxt file (possible to Load in Java)\n",
    "#tfrecord_serving_input_fn = tf.contrib.learn.build_parsing_serving_input_fn(layers.create_feature_spec_for_parsing(feature_columns))  \n",
    "#classifier.export_savedmodel(export_dir_base=\"test\", serving_input_fn = tfrecord_serving_input_fn,as_text=True)\n",
    "\n",
    "\n",
    "# Measure accuracy\n",
    "pred = list(classifier.predict(test_inputs, as_iterable=True))\n",
    "#pred = list(classifier.predict(test_inputs))\n",
    "score = metrics.accuracy_score(test_output, pred)\n",
    "print(\"Final score: {}\".format(score))\n",
    "\n",
    "accuracy_score = classifier.evaluate(x=test_inputs,\n",
    "                                     y=test_output)[\"accuracy\"]\n",
    "\n",
    "print('Accuracy: {0:f}'.format(accuracy_score))\n",
    "\n",
    "# test individual samples \n",
    "sample_1 = np.array( [[0.37671986791414125,0.28395908337619136,-0.0966095873607713,-1.0,0.06891621389763203,-0.09716678086712205,0.726029084013637,4.984689881073479E-4,-0.30296253267499107,-0.16192917054985334,0.04820256230479658,0.4951319883569152,0.5269983894210499,-0.2560313828048315,-0.3710980821053321,-0.4845867212612598,-0.8647234314469595,-0.6491591208322198,-1.0,-0.5004549422844073,-0.9880910165770813,0.5540293108747256,0.5625990251930839,0.7420121698556554,0.5445551415657979,0.4644276850235627,0.7316976292340245,0.636690006814346,0.16486621649984112,-0.0466018967678159,0.5261100063227044,0.6256168612312738,-0.544295484930702,0.379125782517193,0.6959368575211544]], dtype=float)\n",
    "sample_2 = np.array( [[1.0,0.7982741870963959,1.0,-0.46270838239235024,0.040320274521029376,0.443451913224413,-1.0,1.0,1.0,-1.0,0.36689718911339564,-0.13577379160035796,-0.5162916256414466,-0.03373651520104648,1.0,1.0,1.0,1.0,0.786999801054777,-0.43856035121103853,-0.8199093927945158,1.0,-1.0,-1.0,-0.1134921695894473,-1.0,0.6420892436196663,0.7871737734493178,1.0,0.6501788845358409,1.0,1.0,1.0,-0.17586627413625022,0.8817194210401085]], dtype=float)\n",
    "\n",
    "pred = list(classifier.predict(sample_2, as_iterable=True))\n",
    "print(\"Prediction for sample_2 is:{} \".format(pred))\n",
    "\n",
    "pred = list(classifier.predict_proba(sample_2, as_iterable=True))\n",
    "print(\"Prediction for sample_2 is:{} \".format(pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
